{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SPARK : READ, TRANSFORM & PROCESS @ SCALE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Student Name : Nelson VICEL--FARAH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  pyspark official documentation\n",
    "\n",
    "Based on the slides of the course and the spark documentation available online, for each cell, write the spark code that will answer the instructions.\n",
    "\n",
    "You have to follow the tutorials and search over internet in order to get hand's on experience with pyspark, the notebook gives you some suggestions but you are free to try new things and explore the possibilities offered pyspark possibilities.\n",
    "\n",
    "List of tutorial to follow :\n",
    "\n",
    "    - The pyspark API quickstart : https://spark.apache.org/docs/latest/api/python/getting_started/index.html\n",
    "        Please be sure to read the DataFrame API before going further.\n",
    "    - The Spark SQL guide : https://spark.apache.org/docs/latest/sql-getting-started.html\n",
    "        This is the most complete guide on how to use Spark SQL. Be sur to do the getting started and the data sources part\n",
    "\n",
    "Important : Create cells to demonstrate the code you write.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pyspark in /home/miolith/.local/lib/python3.10/site-packages (3.2.1)\n",
      "Requirement already satisfied: py4j==0.10.9.3 in /home/miolith/.local/lib/python3.10/site-packages (from pyspark) (0.10.9.3)\n"
     ]
    }
   ],
   "source": [
    "! pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  pyspark basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_application_name = \"Spark_Application_Name\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/bin/hadoop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/etc/profile.d/perlbin.sh: line 5: append_path: command not found\n",
      "/etc/profile.d/perlbin.sh: line 7: append_path: command not found\n",
      "/etc/profile.d/perlbin.sh: line 9: append_path: command not found\n",
      "WARNING: HADOOP_SLAVES has been replaced by HADOOP_WORKERS. Using value of HADOOP_SLAVES.\n",
      "SLF4J: Class path contains multiple SLF4J bindings.\n",
      "SLF4J: Found binding in [jar:file:/opt/apache-spark/jars/slf4j-log4j12-1.7.30.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/usr/lib/hadoop-3.3.2/share/hadoop/common/lib/slf4j-log4j12-1.7.30.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/opt/apache-spark/jars/spark-unsafe_2.12-3.2.0.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "2022-05-17 22:57:29,348 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = (SparkSession.builder.appName(spark_application_name).getOrCreate())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the createDataFrame function, create a spark dataframe with the values [(\"Brooke\", 20), (\"Denny\", 31), (\"Jules\", 30),(\"TD\", 35), (\"Brooke\", 25)] and the columns [\"name\", \"age\"], give this dataframe the name data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = spark.createDataFrame(data = [(\"Brooke\", 20), (\"Denny\", 31), (\"Jules\", 30),(\"TD\", 35), (\"Brooke\", 25)],\n",
    "                                schema = [\"name\", \"age\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write the line of code that will show the schema of data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write the line of code that will display 3 values of the dataframe data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+\n",
      "|  name|age|\n",
      "+------+---+\n",
      "|Brooke| 20|\n",
      "| Denny| 31|\n",
      "| Jules| 30|\n",
      "+------+---+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_df.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write the line of code that will return a dataframe with 5 values of the data_df dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[name: string, age: bigint]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df.limit(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark gives the possibility to read a wide range of data sources by natively supporting the read of a huge number of file types and databases\n",
    "\n",
    "In the following, write the line of code that will allow you to read the json file specified in the json_file_path variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file_path = \"data/devices.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_df = spark.read.json(json_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write the line of code that will allow you to read a csv file specified in the csv_file_path variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file_path = \"data/devices.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_df = spark.read.csv(csv_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function 1 : read_disp_info()\n",
    "Write a function, named read_disp_info, which takes as input a file path (json or csv path) and give the following information : It creates a dataframe from the file path, display it's schema, display the 10 first line of the dataframe and give the total number of rows of the dataframe\n",
    "It is important to understand that this function should be able to read the json and csv files, to do this, it can look at the end of filepath string in order to deduce if the extension of the file is csv or json and then call the reading function accordinly to the extension. At the end, this function return a spark dataframe object representing the read data. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_disp_info(file_path):\n",
    "    if file_path[-4:] == \".csv\":\n",
    "        df = spark.read.csv(file_path)\n",
    "    elif file_path[-5:] == \".json\":\n",
    "        df = spark.read.json(file_path)\n",
    "    \n",
    "    df.printSchema()\n",
    "    df.show(10)\n",
    "    print(df.count(), 'rows')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this function, read the file present in \"/home/adminefrei/Documents/data/devices.json\" and affect the retuned dataframe to the variable devDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dev_type: string (nullable = true)\n",
      " |-- devnum: long (nullable = true)\n",
      " |-- make: string (nullable = true)\n",
      " |-- model: string (nullable = true)\n",
      " |-- release_dt: string (nullable = true)\n",
      "\n",
      "+--------+------+--------+-----+--------------------+\n",
      "|dev_type|devnum|    make|model|          release_dt|\n",
      "+--------+------+--------+-----+--------------------+\n",
      "|   phone|     1|Sorrento| F00L|2008-10-21T00:00:...|\n",
      "|   phone|     2| Titanic| 2100|2010-04-19T00:00:...|\n",
      "|   phone|     3|  MeeToo|  3.0|2011-02-18T00:00:...|\n",
      "|   phone|     4|  MeeToo|  3.1|2011-09-21T00:00:...|\n",
      "|   phone|     5|  iFruit|    1|2008-10-21T00:00:...|\n",
      "|   phone|     6|  iFruit|    3|2011-11-02T00:00:...|\n",
      "|   phone|     7|  iFruit|    2|2010-05-20T00:00:...|\n",
      "|   phone|     8|  iFruit|    5|2013-07-02T00:00:...|\n",
      "|   phone|     9| Titanic| 1000|2008-10-21T00:00:...|\n",
      "|   phone|    10|  MeeToo|  1.0|2008-10-21T00:00:...|\n",
      "+--------+------+--------+-----+--------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "50 rows\n"
     ]
    }
   ],
   "source": [
    "devDF = read_disp_info(\"data/devices.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataFrame transformations typically return another DataFrame. Try using a select to return a DataFrame with only the make and model columns, name this dataframe makeModelDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "makeModelDF = devDF.select(\"make\", \"model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the schema of the makeModelDF, what can you deduce ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- make: string (nullable = true)\n",
      " |-- model: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "makeModelDF.printSchema() # It creates a new dataframe with a new schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformations in a query can be chained together. Write the line of code that will run a query using select the columns devnum, make, and model and where the make is equal to \"Ronin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+--------------+\n",
      "|devnum| make|         model|\n",
      "+------+-----+--------------+\n",
      "|    15|Ronin|Novelty Note 1|\n",
      "|    17|Ronin|Novelty Note 3|\n",
      "|    18|Ronin|Novelty Note 2|\n",
      "|    19|Ronin|Novelty Note 4|\n",
      "|    46|Ronin|            S4|\n",
      "|    47|Ronin|            S1|\n",
      "|    48|Ronin|            S3|\n",
      "|    49|Ronin|            S2|\n",
      "+------+-----+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "devDF.select(\"devnum\", \"make\", \"model\").where(\"make == 'Ronin'\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark can also read parquet files, parquet is one of the most used format in hdfs, search in the doc and write the line of the code that will let you read the parquet file present in parquet_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "parquet_path = \"data/base_stations.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-------------+-----+-------+---------+\n",
      "| id|  zip|         city|state|    lat|      lon|\n",
      "+---+-----+-------------+-----+-------+---------+\n",
      "|  1|86502|     Chambers|   AZ|35.2375| -109.523|\n",
      "|  2|86514| Teec Nos Pos|   AZ|36.7797| -109.359|\n",
      "|  3|85602|       Benson|   AZ|31.9883|-110.2941|\n",
      "|  4|86011|    Flagstaff|   AZ|35.6308|-112.0524|\n",
      "|  5|86016|Gray Mountain|   AZ|35.6308|-112.0524|\n",
      "|  6|86018|        Parks|   AZ|35.2563|  -111.95|\n",
      "|  7|86336|       Sedona|   AZ|34.8266|-111.7506|\n",
      "|  8|85547|       Payson|   AZ|34.2575|-111.2878|\n",
      "|  9|85548|      Safford|   AZ| 32.797|-109.7522|\n",
      "| 10|85533|      Clifton|   AZ|33.1323|-109.2462|\n",
      "| 11|85922|         Blue|   AZ|33.6512|-109.0685|\n",
      "| 12|85075|      Phoenix|   AZ|33.2765|-112.1872|\n",
      "| 13|85202|         Mesa|   AZ|33.3851|-111.8724|\n",
      "| 14|85254|   Scottsdale|   AZ|33.6165|-111.9554|\n",
      "| 15|85255|   Scottsdale|   AZ|33.6968|-111.8892|\n",
      "| 16|85320|       Aguila|   AZ|33.2765|-112.1872|\n",
      "| 17|85339|       Laveen|   AZ|33.3436|-112.1716|\n",
      "| 18|85381|       Peoria|   AZ|33.6048|-112.2237|\n",
      "| 19|86430|Bullhead City|   AZ|35.1473|-114.5433|\n",
      "| 20|86434|Peach Springs|   AZ|35.5378|-113.4202|\n",
      "+---+-----+-------------+-----+-------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parquet_df = spark.read.parquet(parquet_path)\n",
    "parquet_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, select the id, lat and lon columns where the city = Scottsdale from this dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---------+\n",
      "| id|    lat|      lon|\n",
      "+---+-------+---------+\n",
      "| 14|33.6165|-111.9554|\n",
      "| 15|33.6968|-111.8892|\n",
      "+---+-------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parquet_df.select(\"id\", \"lat\", \"lon\").where(\"city = 'Scottsdale'\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Schema Definition of a DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a new DataFrame based on the devices.json file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "devDF = spark.read.json(json_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View the schema of the devDF DataFrame. Note the column names and types that Spark inferred from the JSON file. In particular, note that the release_dt column is of type string, whereas the data in the column actually represents a timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dev_type: string (nullable = true)\n",
      " |-- devnum: long (nullable = true)\n",
      " |-- make: string (nullable = true)\n",
      " |-- model: string (nullable = true)\n",
      " |-- release_dt: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "devDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining a schema that correctly specifies the column types for this DataFrame starts by importing the package with the definitions of necessary classes and types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create a collection of StructField objects, which represent column definitions. The release_dt column will be a timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "devColumns = [StructField(\"devnum\",LongType()), StructField(\"make\",StringType()), StructField(\"model\",StringType()), StructField(\"release_dt\",TimestampType()),StructField(\"dev_type\",StringType())]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a schema (a StructType object) using the column definition list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "devSchema = StructType(devColumns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that you have created the devSchema, search in the doc how you can specify the schema when you create the dataframe from the json_file_path (\"/path/to/devices.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_df = spark.read.schema(devSchema).json(json_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View the schema and data of the new DataFrame, and confirm that the release_dt column type is now timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- devnum: long (nullable = true)\n",
      " |-- make: string (nullable = true)\n",
      " |-- model: string (nullable = true)\n",
      " |-- release_dt: timestamp (nullable = true)\n",
      " |-- dev_type: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dev_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write the line of code that will read the account_device_path correctly, search how to set up the delimiter and consider the header when reading the csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "account_device_path = \"data/accountdevice/part-00000-f3b62dad-1054-4b2e-81fd-26e54c2ae76a.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+---------+---------------+--------------------+\n",
      "|   id|account_id|device_id|activation_date|   account_device_id|\n",
      "+-----+----------+---------+---------------+--------------------+\n",
      "|48692|     32443|       29|  1393242509000|7351fed1-f344-4cd...|\n",
      "|48693|     32444|        4|  1353649861000|6da22278-ff7a-461...|\n",
      "|48694|     32445|        9|  1331819465000|cb993b85-6775-407...|\n",
      "|48695|     32446|       43|  1336860950000|48ea2c09-a0df-4d1...|\n",
      "|48696|     32446|       29|  1383650663000|4b49c0a6-d141-42e...|\n",
      "+-----+----------+---------+---------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "account_device_df = spark.read.csv(account_device_path, sep=\",\", header=True)\n",
    "account_device_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update the function read_disp_info(), and add to it two optional parameters (header and delimiter) those two optional paramaters can be used optionnally and have the default values header = False and delimiter = \";\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def updated_read_disp_info(file_path, header = False, delimiter = \";\"):\n",
    "    if file_path[-4:] == \".csv\":\n",
    "        df = spark.read.csv(file_path, header=header, sep=delimiter)\n",
    "    elif file_path[-5:] == \".json\":\n",
    "        df = spark.read.json(file_path)\n",
    "    \n",
    "    df.printSchema()\n",
    "    df.show(10)\n",
    "    print(df.count(), 'rows')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, using the function defined, test it to read the account_device_path file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- account_id: string (nullable = true)\n",
      " |-- device_id: string (nullable = true)\n",
      " |-- activation_date: string (nullable = true)\n",
      " |-- account_device_id: string (nullable = true)\n",
      "\n",
      "+-----+----------+---------+---------------+--------------------+\n",
      "|   id|account_id|device_id|activation_date|   account_device_id|\n",
      "+-----+----------+---------+---------------+--------------------+\n",
      "|48692|     32443|       29|  1393242509000|7351fed1-f344-4cd...|\n",
      "|48693|     32444|        4|  1353649861000|6da22278-ff7a-461...|\n",
      "|48694|     32445|        9|  1331819465000|cb993b85-6775-407...|\n",
      "|48695|     32446|       43|  1336860950000|48ea2c09-a0df-4d1...|\n",
      "|48696|     32446|       29|  1383650663000|4b49c0a6-d141-42e...|\n",
      "|48697|     32447|        6|  1342578469000|cc8e8361-3d67-4be...|\n",
      "|48698|     32447|       29|  1386643231000|b40dba90-b073-405...|\n",
      "|48699|     32448|        5|  1350883104000|f088d30f-1e1c-47f...|\n",
      "|48700|     32448|       29|  1383321310000|2805df93-2e89-433...|\n",
      "|48701|     32449|       34|  1333225574000|e0e7edbe-77fc-421...|\n",
      "+-----+----------+---------+---------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "194764 rows\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[id: string, account_id: string, device_id: string, activation_date: string, account_device_id: string]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updated_read_disp_info(account_device_path, True, \",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d31793bc0164bbf1a7493650a68c8e00179ba062bd5a26b747d14cdd740a9f47"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
